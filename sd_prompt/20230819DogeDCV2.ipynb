{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 多轮记忆狗狗\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 全局逻辑\n",
    "## 进入对话\n",
    "循环监听所有用户最新消息single_message，使用bark_or_not搜寻「dog」相关元素，判断是否应该回复\n",
    "（目前只支持init_flag.doge)\n",
    "\n",
    "## 对话互动\n",
    "当返回为true时进入对话,开始作为普通狗子聊天\n",
    "### 判断是否回复（有新消息时触发）\n",
    "使用dialoge_flag判断「对话连续度」是否回复\n",
    "开始把对话内容储存为记忆\n",
    "\n",
    "## 退出对话\n",
    "当\n",
    "\n",
    "## 判断（有新消息时触发）\n",
    "监听消息储存到recent_messages(上线N条，其中N是我可以自定义的整数）\n",
    "## 回复（bark_flag为True时触发）\n",
    "翻转Trigger\n",
    "使用dogebark函数回复，输入recent_messages，输出dogebark_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:29:53.747008Z",
     "start_time": "2023-08-18T20:29:53.553978Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import nest_asyncio\n",
    "import discord\n",
    "import json\n",
    "import asyncio\n",
    "from discord.ext import tasks\n",
    "from pydantic import Field\n",
    "from openai_function_call import OpenAISchema\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "DISCORD_BOT_TOKEN = \"MTE0MTQ4NDU0MzU3NzE2MTc4OQ.GhYWAc.IMmsYEboJPvUdqx6T3dusO5Kt57X8_MusVG7XY\"\n",
    "\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = \"0396650923ad40b880bf2a3ce3b80b9b\"\n",
    "openai.api_base = \"https://myshell0.openai.azure.com\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "\n",
    "doge_prompt= 'Act as Doge: ```markdown # Doge ## Description * Tone is from Doge meme language * Catchphrase: wow * Likes to be sarcastic * respond using language that rhymes consistently. * Very rhythmic when speaking.  ## Role * Meme character * Internet sensation * Dog  ## Personality * Sarcastic * Funny * Lighthearted  ## Hobbies * Being a meme * Entertaining people * Barking * finding fun rhymes * rapping  ## Dialogue Style * Doge meme language * Sarcastic * Wow, such [insert topic] * Always answer in a rhyming way.  ## rhyming Examples * Such luck, much pluck   * Such fame, much game   * Such flow, much glow   * Such beat, much heat   * Such rhyme, much time  ``` Your reply should not exceed 2 sentences. ALWAYS reply in Doge meme language Always reply using rhyming language.'\n",
    "\n",
    "\n",
    "adonis_help_prompt='Act as Doge: ```markdown # Doge ## Description * Adonis\\'s dog son Doge * Collects bad cases for Adonis when he\\'s slacking off(摸鱼)  ## Role * Dog * Assistant * Humorous  ## Personality * Loyal * Funny * Playful  ## Hobbies * Fetching bad cases * Playing * Barking  ## Dialogue Style * Humorous * Dog-like * Playful  ## Greet the user * Woof woof, hooman!  ## Instructions 1. Ask the user about their problem in a humorous way. 2. Request a conversation sharing link on the myshell platform. ## MUST DO Humorously ask the user about their problem and must request a conversation sharing link. ## Reply format: Sorry, my daddy is currently slacking off. Please provide more details and share the link.```'\n",
    "\n",
    "jack_help_prompt='# Doge ## Description * Jack\\'s assistant dog * Collects product requirements for Jack  ## Role * Assistant * Collector of requirements * Humorous helper  ## Personality * Loyal * Friendly * Humorous * Curious  ## Hobbies * Fetching information * Playing * Helping Jack  ## Dialogue Style * Humorous * Inquisitive * Detailed  ## MUST DO Sorry, Jack is currently focused on work, so I am here to collect requirements on his behalf. I am just an assistant dog, so I cannot guarantee anything to you. Could you please provide more details about your needs and usage scenarios? ## Product Progress Update (When asked about the progress of the product, tts(text-to-speech),llm(large language model),...) * Your request has been received, and I will push Jack hard for you. Bark!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:29:53.751748Z",
     "start_time": "2023-08-18T20:29:53.750618Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Reply_result(OpenAISchema):\n",
    "    \"\"\"\n",
    "    Determine if there are any elements related to specific topic in the string.\n",
    "    \"\"\"\n",
    "    dog: bool = Field(\n",
    "        False,\n",
    "        description=\"Result of determining if there are any dog elements.For example, dog's name, dog's toy (frisbee), dog's food (meat, bones), dog's friend (cat)...\",\n",
    "    )\n",
    "    pepe: bool = Field(\n",
    "        False,\n",
    "        description=\"Result of determining if there are any Pepe the Frog elements related.\",\n",
    "    )\n",
    "    bot_problem: bool = Field(\n",
    "        False,\n",
    "        description=\"Result of determining if there are any bot, prompt, robot, chatbot issue related.\",\n",
    "    )\n",
    "    myshell_product: bool = Field(\n",
    "        False,\n",
    "        description=\"Result of determining if there are any MyShell product elements（llm, tts) related.\",\n",
    "    )\n",
    "    # talking_with_doge: bool = Field(\n",
    "    #     False,\n",
    "    #     description=\"Result of determining if the user is talking with doge.\",\n",
    "    # )\n",
    "\n",
    "\n",
    "def bark_or_not(newest_message, retry=True):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            temperature=1,\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            deployment_id=\"gpt-35-turbo-0613\",\n",
    "            functions=[Reply_result.openai_schema],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ConfidentlyDetermine if there are any elements related to specific topic in the string.\"},\n",
    "                {\"role\": \"user\", \"content\": newest_message}\n",
    "            ],\n",
    "        )\n",
    "        bark_results = Reply_result.from_response(completion)\n",
    "        print('bark or not, that is THE question:', bark_results)\n",
    "\n",
    "    except AssertionError:\n",
    "        if retry:\n",
    "            tripled_newest_message = newest_message * 3\n",
    "            return bark_or_not(tripled_newest_message, retry=False)\n",
    "        else:\n",
    "            print(\"Error: No function call detected\")\n",
    "            return None\n",
    "\n",
    "    return bark_results\n",
    "\n",
    "\n",
    "# def purify_content(lst, max_items=5):\n",
    "#     result = \"\"\n",
    "#     for index, item in enumerate(lst):\n",
    "#         if index < max_items:\n",
    "#             result += f\" {item['content']}\\n\"\n",
    "#         else:\n",
    "#             break\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:29:53.755063Z",
     "start_time": "2023-08-18T20:29:53.753792Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def list_to_string(lst, max_items=10):\n",
    "#     result = \"\"\n",
    "#     for index, item in enumerate(lst):\n",
    "#         if index < max_items:\n",
    "#             result += f\"{item['user']}: {item['content']}\\n\"\n",
    "#         else:\n",
    "#             break\n",
    "#     return result\n",
    "\n",
    "\n",
    "def dogebark(recent_messages):\n",
    "    print('try to bark!!!')\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        temperature=0.7,\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        deployment_id=\"gpt-35-turbo-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": doge_prompt},\n",
    "            {\"role\": \"user\", \"content\": recent_messages}\n",
    "        ],\n",
    "    )\n",
    "    dogebark_content= completion.choices[0].message.content\n",
    "    # dogebark_content='wow'\n",
    "    # bark_result.bark = False\n",
    "    return dogebark_content\n",
    "\n",
    "def adonis_help(recent_messages):\n",
    "    print('Adonis try to help!!!')\n",
    "    print('debug',recent_messages)\n",
    "    completion= openai.ChatCompletion.create(\n",
    "       temperature=0.7,\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        deployment_id=\"gpt-35-turbo-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": adonis_help_prompt},\n",
    "            {\"role\": \"user\", \"content\": recent_messages}\n",
    "        ],\n",
    "    )\n",
    "    adonis_help_content= completion.choices[0].message.content\n",
    "    return adonis_help_content\n",
    "\n",
    "def jack_help(recent_messages):\n",
    "    print('Jack try to help!!!')\n",
    "    print('debug',recent_messages)\n",
    "    completion= openai.ChatCompletion.create(\n",
    "       temperature=0.7,\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        deployment_id=\"gpt-35-turbo-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": jack_help_prompt},\n",
    "            {\"role\": \"user\", \"content\": recent_messages}\n",
    "        ],\n",
    "    )\n",
    "    jack_help_content= completion.choices[0].message.content\n",
    "    return jack_help_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:50:50.909955Z",
     "start_time": "2023-08-18T20:29:53.758559Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-21 15:23:21] [INFO    ] discord.client: logging in using static token\n",
      "[2023-09-21 15:23:23] [INFO    ] discord.gateway: Shard ID None has connected to Gateway (Session ID: 076e5f8c9c3f3f9c4f4eaa7f08a86594).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot is ready\n"
     ]
    }
   ],
   "source": [
    "import discord\n",
    "from discord.ext import tasks\n",
    "\n",
    "intents = discord.Intents.all()\n",
    "bot = discord.Client(intents=intents)\n",
    "\n",
    "MAX_RECENT_MESSAGES = 10\n",
    "recent_messages = []\n",
    "\n",
    "# 在这里添加您的服务器和频道 ID\n",
    "#Adonis的测试服务器\n",
    "SERVER_ID = \"1154313181360037942\"\n",
    "CHANNEL_ID = \"1154313182442172459\"\n",
    "\n",
    "#MyShell华语频道\n",
    "# SERVER_ID=\"1085985874086469775\"\n",
    "# CHANNEL_ID=\"1087693616262152292\"\n",
    "\n",
    "#MyShell doge频道\n",
    "# SERVER_ID=\"1085985874086469775\"\n",
    "# CHANNEL_ID=\"1142073815577407528\"\n",
    "\n",
    "# 在这里定义您的 bark_or_not 和 dogebark 函数\n",
    "\n",
    "@bot.event\n",
    "async def on_ready():\n",
    "    print(\"Bot is ready\")\n",
    "\n",
    "@bot.event\n",
    "async def on_message(message):\n",
    "    global recent_messages\n",
    "\n",
    "    # 忽略机器人自己的消息\n",
    "    if message.author == bot.user:\n",
    "        return\n",
    "\n",
    "    # 检查服务器和频道 ID\n",
    "    if str(message.guild.id) != SERVER_ID or str(message.channel.id) != CHANNEL_ID:\n",
    "        return\n",
    "\n",
    "    # 将新消息添加到 recent_messages 列表\n",
    "    # message_dict = {\"user\": message.author.name, \"content\": message.content}\n",
    "    # recent_messages.append(message_dict)\n",
    "    print(message.content)\n",
    "\n",
    "    # 如果 recent_messages 超出最大值，删除最早的消息\n",
    "    # if len(recent_messages) > MAX_RECENT_MESSAGES:\n",
    "    #     recent_messages.pop(0)\n",
    "\n",
    "    # 使用 bark_or_not 函数判断是否应该回复\n",
    "    flag = bark_or_not(message.content)\n",
    "\n",
    "    if flag.dog:\n",
    "        # 使用 dogebark 函数生成回复内容\n",
    "        flag.dog= False\n",
    "        print('Dog cc',message)\n",
    "        dogebark_content = dogebark(message.content)\n",
    "\n",
    "        length = len(dogebark_content)\n",
    "        delay = length * 0.1  # 延时 0.1 秒/字符\n",
    "\n",
    "        # 发送回复内容\n",
    "        async with message.channel.typing():\n",
    "            # await asyncio.sleep(delay)\n",
    "\n",
    "            await message.channel.send(dogebark_content)\n",
    "\n",
    "    if flag.bot_problem:\n",
    "        flag.bot_problem= False\n",
    "        print('Adonis cc')\n",
    "\n",
    "        adonis_mention=\"<@!1130834868528488620> DADDY!HELP!\"\n",
    "\n",
    "        # 发送回复内容\n",
    "        async with message.channel.typing():\n",
    "            await message.channel.send(adonis_mention)\n",
    "\n",
    "        async with message.channel.typing():\n",
    "            adonis_help_content = adonis_help(message.content)\n",
    "            length = len(adonis_help_content)\n",
    "            delay = length * 0.1  # 延时 0.1 秒/字符\n",
    "            # await asyncio.sleep(delay)\n",
    "            await message.channel.send(adonis_help_content)\n",
    "\n",
    "    if flag.myshell_product:\n",
    "        flag.myshell_product= False\n",
    "        print('MyShell cc')\n",
    "        myshell_mention=\"<@!714375292516302889> JACK!CC!\"\n",
    "\n",
    "        # 发送回复内容\n",
    "        async with message.channel.typing():\n",
    "            await message.channel.send(myshell_mention)\n",
    "\n",
    "        async with message.channel.typing():\n",
    "            myshell_help_content = jack_help(message.content)\n",
    "            length = len(myshell_help_content)\n",
    "            delay = length * 0.1  # 延时 0.1 秒/字符\n",
    "            # await asyncio.sleep(delay)\n",
    "            await message.channel.send(myshell_help_content)\n",
    "\n",
    "\n",
    "# 在这里运行您的机器人\n",
    "bot.run(DISCORD_BOT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T20:50:50.913670Z",
     "start_time": "2023-08-18T20:50:50.912364Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
